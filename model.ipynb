{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "1",
            "metadata": {},
            "source": [
                "# ðŸ§ª CookMate AI: Qwen-VL Backend (Memory Optimized)\n",
                "This version uses **4-bit quantization** to fit comfortably in Colab's T4 GPU memory.\n",
                "\n",
                "### Instructions:\n",
                "1. Run the cells below.\n",
                "2. Copy the **Ngrok URL** at the end.\n",
                "3. Paste into website settings.\n",
                "\n",
                "**Fixes implemented:**\n",
                "- 4-bit Quantization (saves ~75% GPU memory)\n",
                "- Automatic Cache Clearing after every detection\n",
                "- Image resolution capping (prevents OOM on large images)\n",
                "- Explicit CORS handling for website connection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Optimized Dependencies\n",
                "!pip install -q transformers torch torchvision qwen-vl-utils accelerate flask flask-cors pyngrok flash-attn bitsandbytes --no-build-isolation\n",
                "!pip install -q git+https://github.com/huggingface/transformers.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load Model with 4-bit Quantization\n",
                "import torch\n",
                "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
                "from qwen_vl_utils import process_vision_info\n",
                "import os\n",
                "import gc\n",
                "\n",
                "os.environ[\"HF_HUB_READ_TIMEOUT\"] = \"300\"\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
                "\n",
                "print(\"Loading Model in 4-bit mode...\")\n",
                "\n",
                "quantization_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
                "    model_id, \n",
                "    device_map=\"auto\",\n",
                "    quantization_config=quantization_config\n",
                ")\n",
                "processor = AutoProcessor.from_pretrained(model_id)\n",
                "print(\"Model Ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. API Server Setup\n",
                "from flask import Flask, request, jsonify, make_response\n",
                "from flask_cors import CORS\n",
                "import base64\n",
                "from PIL import Image\n",
                "import io\n",
                "from pyngrok import ngrok\n",
                "\n",
                "app = Flask(__name__)\n",
                "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
                "\n",
                "@app.after_request\n",
                "def add_header(response):\n",
                "    response.headers['Access-Control-Allow-Origin'] = '*'\n",
                "    response.headers['Access-Control-Allow-Headers'] = 'Content-Type,Authorization,ngrok-skip-browser-warning'\n",
                "    response.headers['Access-Control-Allow-Methods'] = 'GET,PUT,POST,DELETE,OPTIONS'\n",
                "    return response\n",
                "\n",
                "@app.route('/ping', methods=['GET', 'OPTIONS'])\n",
                "def ping():\n",
                "    if request.method == 'OPTIONS': return make_response('', 200)\n",
                "    return jsonify({\"status\": \"alive\"})\n",
                "\n",
                "@app.route('/detect', methods=['POST', 'OPTIONS'])\n",
                "def detect():\n",
                "    if request.method == 'OPTIONS': return make_response('', 200)\n",
                "        \n",
                "    data = request.json\n",
                "    if not data or 'image' not in data:\n",
                "        return jsonify({\"error\": \"No image provided\"}), 400\n",
                "\n",
                "    try:\n",
                "        # Decode Image and Resize to prevent OOM\n",
                "        image_data = base64.b64decode(data['image'])\n",
                "        image = Image.open(io.BytesIO(image_data))\n",
                "        image.thumbnail((1280, 1280)) # Scale down large images\n",
                "        \n",
                "        # Inference\n",
                "        messages = [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": \"Identify raw food ingredients, vegetables, or fruits. Output ONLY the names as a plain comma-separated list. No intro. Example: Tomato, Onion\"\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": \"List the ingredients.\"}]\n",
                "            }\n        ]\n",
                "        \n",
                "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "        image_inputs, _ = process_vision_info(messages)\n",
                "        inputs = processor(text=[text], images=image_inputs, padding=True, return_tensors=\"pt\").to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
                "        \n",
                "        output_text = processor.batch_decode(\n",
                "            [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)], \n",
                "            skip_special_tokens=True\n",
                "        )[0]\n",
                "\n",
                "        # Cleanup memory immediately\n",
                "        del inputs, generated_ids\n",
                "        gc.collect()\n",
                "        torch.cuda.empty_cache()\n",
                "\n",
                "        # Parse results\n",
                "        clean_text = output_text.replace(\"Detected:\", \"\").replace(\"*\", \"\").replace(\"-\", \"\")\n",
                "        ingredients = [idx.strip() for idx in clean_text.split(\",\") if idx.strip()]\n",
                "        print(f\"Detected: {ingredients}\")\n",
                "        \n",
                "        return jsonify({\"ingredients\": ingredients})\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        torch.cuda.empty_cache()\n",
                "        return jsonify({\"error\": str(e)}), 500\n",
                "\n",
                "# 4. Start Tunnel and Run\n",
                "print(\"Enter Ngrok Auth Token (or set in Colab Secrets as 'NGROK_AUTH_TOKEN'):\")\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
                "    if auth_token:\n",
                "        print(\"Loaded token from Colab Secrets.\")\n",
                "except:\n",
                "    auth_token = input(\"Paste your Ngrok Auth Token: \")\n",
                "\n",
                "if auth_token:\n",
                "    ngrok.set_auth_token(auth_token)\n",
                "\n",
                "try:\n",
                "    public_url = ngrok.connect(5000, bind_tls=True).public_url\n",
                "    print(f\"\\nðŸš€ SERVER LIVE AT: {public_url}\")\n",
                "except Exception as e:\n",
                "    print(f\"Ngrok error: {e}\")\n",
                "\n",
                "app.run(host='0.0.0.0', port=5000)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}